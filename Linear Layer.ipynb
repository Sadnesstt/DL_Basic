{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67134198",
   "metadata": {},
   "source": [
    "## Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d864f009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a0efbf",
   "metadata": {},
   "source": [
    "## Raw Linear Layer\n",
    "$$ y = x \\cdot W + b,$$\n",
    "$$ where x \\in \\mathbb{R}^{N \\times n}, y \\in \\mathbb{R}^{N \\times m}, $$\n",
    "$$ Thus, W \\in \\mathbb{R}^{N \\times m} and b \\in \\mathbb{R}^{m}. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d33a8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.FloatTensor([[1, 2],\n",
    "                      [3, 4],\n",
    "                      [5, 6]])\n",
    "b = torch.FloatTensor([2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e7df22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "print(W.size())\n",
    "print(b.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b268f0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x, W, b):\n",
    "    y = torch.matmul(x, W) + b\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce020d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([[1, 1, 1],\n",
    "                      [2, 2, 2],\n",
    "                      [3, 3, 3],\n",
    "                      [4, 4, 4]])\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc241b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = linear(x, W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd8e2d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "print(y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2979fd79",
   "metadata": {},
   "source": [
    "## nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "385fba80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn # 굉장히 유용하게 사용하는것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d5a2203",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module): ## nn.Module을 상속함.\n",
    "    \n",
    "    def __init__(self, input_dim = 3, output_dim = 2):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.W = torch.FloatTensor(input_dim, output_dim)\n",
    "        self.b = torch.FloatTensor(output_dim)\n",
    "        \n",
    "    # You should override 'forward' method to implement detail.\n",
    "    # The input argument and outputs can be designed as you wish.\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # |x| = (batch_size, input_dim)\n",
    "        y = torch.matmul(x, self.W) + self.b\n",
    "        # |y| = (batch_size, input_dim) * (input_dim, output_dim)\n",
    "        #     = (batch_size, output_dim)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "366355f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = MyLinear(3, 2)\n",
    "y = linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abb206f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b803afc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in linear.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c110380",
   "metadata": {},
   "source": [
    "You can see that there is no weight parameters to learn. Above way can forward(or calculate) values, but it cannot be trained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d09ec90",
   "metadata": {},
   "source": [
    "## Correct way: nn.Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e8868ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim = 3, output_dim = 2):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.W = nn.Parameter(torch.FloatTensor(input_dim, output_dim))\n",
    "        self.b = nn.Parameter(torch.FloatTensor(output_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # |x| = (batch_size, input_dim)\n",
    "        y = torch.matmul(x, self.W) + self.b\n",
    "        # |y| = (batch_size, input_dim) * (input_dim, output_dim)\n",
    "        #     = (batch_size, output_dim)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a51a1c",
   "metadata": {},
   "source": [
    "Reference: https://pytorch.org/docs/stable/nn.html/#torch.nn.Parameter\n",
    "\n",
    "A kind of Tensor that is to be considered a module parameter\n",
    "\n",
    "Parameters are Tensor subclasses, that have a very special property when used with Modules - when they're assigned as Module attributes they are automatically added to the list of its parameters, and will appear e.g. in paramters() iterator.\n",
    "Assigning a Tensor doesn't have such effect. This is because one might want to cache some temporary state, like last hidden state of the RNN, in the model. If there was no such class as Parameter, these temporaries would get registered too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdeb6aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = MyLinear(3, 2)\n",
    "y = linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "006c28ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "028c333e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.0000e+00, 1.4586e-19],\n",
      "        [4.2729e-05, 1.3341e-08],\n",
      "        [6.7354e+22, 3.2908e+21]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([9.1477e-41, 0.0000e+00], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p in linear.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c87723",
   "metadata": {},
   "source": [
    "## nn.Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34f3b378",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Linear(3, 2)\n",
    "y = linear(x) # 자동으로 forward를 불러와서 fc를 해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7a8c202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3f65490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3150,  0.3283,  0.1125],\n",
      "        [-0.1527, -0.4191, -0.2179]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3573,  0.5327], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p in linear.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da405aea",
   "metadata": {},
   "source": [
    "## nn.Module can contain other nn.Module's child classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32d0ef7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim = 3, output_dim = 2):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # |x| = (batch_size, input_dim)\n",
    "        y = self.linear(x)\n",
    "        # |y| = (batch_size, output_dim)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "286c0c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = MyLinear(3, 2)\n",
    "y = linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1356108a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f3208cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.nn' has no attribute 'Parameters'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mParameters\u001b[49m():\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(p)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch.nn' has no attribute 'Parameters'"
     ]
    }
   ],
   "source": [
    "for p in nn.Parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e361957d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
